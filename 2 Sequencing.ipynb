{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75f4949-a2da-43f8-865e-f781533fdbdb",
   "metadata": {},
   "source": [
    "## Sequencing : Turning sentences into data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0b1ce-1183-49c3-ba32-921f379766ca",
   "metadata": {},
   "source": [
    "#### Hemant Thapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404c6a1-3ad5-4173-90dc-e1a2d2cbf53e",
   "metadata": {},
   "source": [
    "The process of converting text data into a sequence of numerical values.  Sequencing converts text - which could be words, sentences, or even entire documents - into a numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bf671-7c1a-45c2-a882-c9134ed746ac",
   "metadata": {},
   "source": [
    "- Tokenization: This is the first step, where the text is split into smaller units called tokens. Tokens can be words, characters, or subwords. For example, the sentence \"Hello world\" might be broken down into [\"Hello\", \"world\"].\n",
    "\n",
    "- Assigning Numeric Values: Each unique token is assigned a specific numeric value. This process creates a mapping where each word or character is represented by a unique number.\n",
    "\n",
    "- Creating Sequences: Once each token has a numeric value, the text can be converted into a sequence of numbers. For instance, if \"Hello\" is assigned the number 1 and \"world\" the number 2, the sentence \"Hello world\" would become [1, 2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "349239ea-9bde-4108-ad76-734b82c73b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5cf4249-d259-4bb7-b403-f1a0cc020f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of string\n",
    "sentences = [\n",
    "    'I love to read books',\n",
    "    'I love to travel around world',\n",
    "    'Do you love reading books!',\n",
    "    'What is your best read ?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec3f9ab-77fb-49a9-86db-62004766d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love to read books', 'I love to travel around world', 'Do you love reading books!', 'What is your best read ?']\n"
     ]
    }
   ],
   "source": [
    "#printing sentences \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e4b803-e1fb-4b97-a5f5-1a99fcb9f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an object \n",
    "tokenizer = Tokenizer(num_words = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f382fad6-ef37-451e-bc30-083b318970f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model \n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9538e09-d54c-4e71-b016-4d5debe52c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictonary with index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "357a12fa-e21c-4ca0-9ac4-c879498b900d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'i': 2, 'to': 3, 'read': 4, 'books': 5, 'travel': 6, 'around': 7, 'world': 8, 'do': 9, 'you': 10, 'reading': 11, 'what': 12, 'is': 13, 'your': 14, 'best': 15}\n"
     ]
    }
   ],
   "source": [
    "#print dictonary \n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7fac08-c3e2-498f-82f4-9a6db35b0a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> love\n",
      "2 -> i\n",
      "3 -> to\n",
      "4 -> read\n",
      "5 -> books\n",
      "6 -> travel\n",
      "7 -> around\n",
      "8 -> world\n",
      "9 -> do\n",
      "10 -> you\n",
      "11 -> reading\n",
      "12 -> what\n",
      "13 -> is\n",
      "14 -> your\n",
      "15 -> best\n"
     ]
    }
   ],
   "source": [
    "for i,j in word_index.items():\n",
    "    print(j, \"->\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b85d52dc-03cb-4bf8-9acc-aeb0adda3f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with sequences \n",
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b289d965-3da9-4b5e-a6ab-657a529eff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 3, 4, 5], [2, 1, 3, 6, 7, 8], [9, 10, 1, 11, 5], [12, 13, 14, 15, 4]]\n"
     ]
    }
   ],
   "source": [
    "#prinitng sequences \n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b57759-506b-4cbf-b107-1388f471abfe",
   "metadata": {},
   "source": [
    "#### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e38bc055-59ff-4331-84d6-6b6065856965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of string\n",
    "test_data = [\n",
    "    'I really love to read books and always prefer general knowledge over fictions',\n",
    "    'I love to travel around world and have a dream to visit seven wonder around the world',\n",
    "    'Do you enjoy reading books!',\n",
    "    'What is your best read till now and what would you recommend?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1247b4c-9ee9-4aab-ba45-abf2933d929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11610f2-526e-4752-b5c3-1ede5988de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 3, 4, 5], [2, 1, 3, 6, 7, 8, 3, 7, 8], [9, 10, 11, 5], [12, 13, 14, 15, 4, 12, 10]]\n"
     ]
    }
   ],
   "source": [
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9c56f63-8389-4c46-b61a-ada9c2e6a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_word_index(word_index):\n",
    "    inverse_word_index = {}\n",
    "    for key, value in word_index.items():\n",
    "        inverse_word_index[value] = key\n",
    "    return inverse_word_index\n",
    "    \n",
    "def decode_text(test_seq, inverse_word_index):\n",
    "    decoded_texts = []\n",
    "    \n",
    "    #loop through each sequence in test_seq\n",
    "    for seq in test_seq:\n",
    "        decoded_seq = \"\"\n",
    "        \n",
    "        #loop through each index in the sequence\n",
    "        for i in seq:\n",
    "            #find the word corresponding to the index\n",
    "            #using ? as a placeholder for missing words\n",
    "            word = inverse_word_index.get(i, '?')  \n",
    "            \n",
    "            #append the word to the decoded sequence\n",
    "            #adding a space for separation between words\n",
    "            decoded_seq += word + \" \"  \n",
    "    \n",
    "        #append the decoded sequence to the list of decoded texts\n",
    "        #using strip() to remove trailing spaces\n",
    "        decoded_texts.append(decoded_seq.strip())  \n",
    "        \n",
    "    for i in decoded_texts:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4688c33c-0591-47d2-a849-82fd290a50dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'love',\n",
       " 2: 'i',\n",
       " 3: 'to',\n",
       " 4: 'read',\n",
       " 5: 'books',\n",
       " 6: 'travel',\n",
       " 7: 'around',\n",
       " 8: 'world',\n",
       " 9: 'do',\n",
       " 10: 'you',\n",
       " 11: 'reading',\n",
       " 12: 'what',\n",
       " 13: 'is',\n",
       " 14: 'your',\n",
       " 15: 'best'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_word_index(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7347dad-8cd1-41a9-ae07-071405f2a2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love to read books\n",
      "i love to travel around world to around world\n",
      "do you reading books\n",
      "what is your best read what you\n"
     ]
    }
   ],
   "source": [
    "decode_text(test_seq, inverse_word_index(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063f14c-2a22-4cb7-88c1-ecd80a26501d",
   "metadata": {},
   "source": [
    "#### Out Of Vocabulary token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3583ef-09bb-4584-8251-b52560a56aca",
   "metadata": {},
   "source": [
    "When you tokenize text - convert it into a series of tokens (like words or characters) - you usually have a fixed vocabulary: a set list of tokens that your model recognizes based on the training data. However, when your model encounters a new word in new or unseen data that wasn't in the training vocabulary, it's considered an out-of-vocabulary word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b162e-2586-405f-9ace-591d8be68bbf",
   "metadata": {},
   "source": [
    "- Placeholder for Unknown Words: The oov_token is a special token that is used as a placeholder for words that are not in the tokenizer's vocabulary. It's a way to handle these unknown words.\n",
    "\n",
    "- Consistency in Tokenization: Without an oov_token, any word not in the vocabulary would be completely ignored during tokenization, leading to loss of information. With an oov_token, you maintain the structure and length of your text data.\n",
    "\n",
    "- Common Usage in Tokenizers: Many tokenization tools, including those in popular libraries like Keras, allow you to specify an oov_token. For example, when creating a tokenizer in Keras, you can set oov_token=\"<OOV>\". Then, during tokenization, any word not found in the word index is replaced by this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a294d9-39ff-4e7c-9033-77ed7f3cabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#objaect with out of vocabulary token\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "078e9b7b-ee50-4fc5-90ab-4e9b84f3d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c19f118-cf76-48c3-9acb-465aa3e11b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictonary with index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb25db80-5ff8-4e3b-ac43-95e6bc197fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> <OOV>\n",
      "2 -> love\n",
      "3 -> i\n",
      "4 -> to\n",
      "5 -> read\n",
      "6 -> books\n",
      "7 -> travel\n",
      "8 -> around\n",
      "9 -> world\n",
      "10 -> do\n",
      "11 -> you\n",
      "12 -> reading\n",
      "13 -> what\n",
      "14 -> is\n",
      "15 -> your\n",
      "16 -> best\n"
     ]
    }
   ],
   "source": [
    "#printing key and values\n",
    "for i,j in word_index.items():\n",
    "    print(j, \"->\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29c51e2e-60d7-4c0b-9df8-0d43d5db4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text to sequences \n",
    "test_seq = tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99e23b6f-e6cf-4512-a77e-cadff6ec4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4, 5, 6, 1, 1, 1, 1, 1, 1, 1], [3, 2, 4, 7, 8, 9, 1, 1, 1, 1, 4, 1, 1, 1, 8, 1, 9], [10, 11, 1, 12, 6], [13, 14, 15, 16, 5, 1, 1, 1, 13, 1, 11, 1]]\n"
     ]
    }
   ],
   "source": [
    "#printing sequences \n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45409b38-4b55-4eca-866e-f0793cb71d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<OOV>',\n",
       " 2: 'love',\n",
       " 3: 'i',\n",
       " 4: 'to',\n",
       " 5: 'read',\n",
       " 6: 'books',\n",
       " 7: 'travel',\n",
       " 8: 'around',\n",
       " 9: 'world',\n",
       " 10: 'do',\n",
       " 11: 'you',\n",
       " 12: 'reading',\n",
       " 13: 'what',\n",
       " 14: 'is',\n",
       " 15: 'your',\n",
       " 16: 'best'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_word_index(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e296ecf-941a-4f6d-9ecf-347de2711879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i <OOV> love to read books <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "i love to travel around world <OOV> <OOV> <OOV> <OOV> to <OOV> <OOV> <OOV> around <OOV> world\n",
      "do you <OOV> reading books\n",
      "what is your best read <OOV> <OOV> <OOV> what <OOV> you <OOV>\n"
     ]
    }
   ],
   "source": [
    "decode_text(test_seq, inverse_word_index(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ddc23d-d7c8-40fd-ad99-24e01020e2a6",
   "metadata": {},
   "source": [
    "#### Padding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60531e81-af51-4fed-a03b-dc3fa10dd971",
   "metadata": {},
   "source": [
    "Technique of standardising the lengths of sequences (like sentences or paragraphs) to be the same size. This is important because models, particularly those in deep learning, require inputs of a consistent size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a182c52-a4a6-4ee5-8bd5-94bb8fa97171",
   "metadata": {},
   "source": [
    "- Handling Variable Lengths: Text data often comes in varying lengths (different number of words or characters in different sentences). However, models like neural networks require input data to be of a fixed size.\n",
    "\n",
    "- Batch Processing: When training models, it's efficient to process data in batches. Padding ensures all sequences in a batch have the same length, allowing for efficient batch processing.\n",
    "\n",
    "- Adding Extra Values: Padding involves adding extra values to sequences to make them all the same length. The padding value is typically 0 but can be set to other values depending on the context and requirements.\n",
    "  \n",
    "- Pre-padding vs. Post-padding: Padding can be added either at the beginning (pre-padding) or the end (post-padding) of the sequences. The choice depends on the model and the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6960b-5496-4561-b271-161899f81ace",
   "metadata": {},
   "source": [
    "Sentence 1: [\"I\", \"love\", \"cats\"]\n",
    "\r\n",
    "Sentence 2: [\"Dogs\", \"are\", \"great\", \"pets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb217e1-beec-42fc-b964-320261d647f4",
   "metadata": {},
   "source": [
    "If converted to sequences with numerical values, you might have:\n",
    "\n",
    "Sentence 1: [5, 12, 7]\n",
    "\n",
    "Sentence 2: [8, 3, 9, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95321e2-bacb-4105-8b9a-eaa38a679e96",
   "metadata": {},
   "source": [
    "If you decide each sequence should have a length of 5 for model input, you'd pad them like this:\n",
    "\n",
    "Sentence 1 with post-padding: [5, 12, 7, 0, 0]\n",
    "\n",
    "Sentence 2 with post-padding: [8, 3, 9, 10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc7d6a80-9431-40bd-88b1-5a3a041905f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d9c9556-0b64-4aea-8155-87223c6a6c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  3  1  2  4  5  6  1  1  1  1  1  1  1]\n",
      " [ 3  2  4  7  8  9  1  1  1  1  4  1  1  1  8  1  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 10 11  1 12  6]\n",
      " [ 0  0  0  0  0 13 14 15 16  5  1  1  1 13  1 11  1]]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1937d011-c87f-4afd-9106-0e1143962623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero after sentences with padding parameter post\n",
    "post_padded = pad_sequences(test_seq, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2679418-aaac-4a80-a719-8da9c287c7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  1  2  4  5  6  1  1  1  1  1  1  1  0  0  0  0]\n",
      " [ 3  2  4  7  8  9  1  1  1  1  4  1  1  1  8  1  9]\n",
      " [10 11  1 12  6  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 14 15 16  5  1  1  1 13  1 11  1  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(post_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abc41183-da22-40c8-8ca9-a764efdf0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero after sentences with padding parameter post, also including max len ten\n",
    "post_padded_max_len = pad_sequences(test_seq, padding='post', maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "150882e8-f6cc-42ce-8ebe-c56b3be9e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5  6  1  1  1  1  1  1  1]\n",
      " [ 1  1  1  4  1  1  1  8  1  9]\n",
      " [10 11  1 12  6  0  0  0  0  0]\n",
      " [15 16  5  1  1  1 13  1 11  1]]\n"
     ]
    }
   ],
   "source": [
    "print(post_padded_max_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a168da49-9831-4554-9198-b98b776cd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_padded = pad_sequences(test_seq, padding='post', truncating='post', maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18dec3cb-1ba0-4688-87d9-bda2879a777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  1  2  4  5  6  1  1  1  1]\n",
      " [ 3  2  4  7  8  9  1  1  1  1]\n",
      " [10 11  1 12  6  0  0  0  0  0]\n",
      " [13 14 15 16  5  1  1  1 13  1]]\n"
     ]
    }
   ],
   "source": [
    "print(temp_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1cb7341-3f81-4963-b630-e37ab4ba181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i <OOV> love to read books <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
      "i love to travel around world <OOV> <OOV> <OOV> <OOV> to <OOV> <OOV> <OOV> around <OOV> world\n",
      "do you <OOV> reading books\n",
      "what is your best read <OOV> <OOV> <OOV> what <OOV> you <OOV>\n"
     ]
    }
   ],
   "source": [
    "#inverse_word_index function to get the dictionary\n",
    "inverse_index_dict = inverse_word_index(word_index)\n",
    "\n",
    "#using this dictionary to decode the sequences\n",
    "for seq in padded:\n",
    "    words = [inverse_index_dict.get(i, '') for i in seq if i != 0]  # Exclude padding\n",
    "    sentence = ' '.join(words).strip()\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
