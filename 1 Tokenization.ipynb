{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575b0162-cd28-4220-9013-215291de07c0",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f9de0-12bb-4135-bdb8-6318a3afc23f",
   "metadata": {},
   "source": [
    "#### Hemant Thapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df01145-23f3-4c28-bfff-01a17c6fc174",
   "metadata": {},
   "source": [
    "Tokenization in the context of natural language processing (NLP) is a foundational step where text is broken down into smaller units called tokens. These tokens are often words, but they can also include smaller units like subwords, characters, or even n-grams (combinations of n words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c2cd2-d30f-4301-bd0c-949813c99d8c",
   "metadata": {},
   "source": [
    "##### Purpose of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc9c25-e34c-4e7c-8481-f242211b900e",
   "metadata": {},
   "source": [
    "Tokenization is a necessary preprocessing step for many NLP tasks. It helps in simplifying the process of analysing text by reducing it to a sequence of manageable pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f660ed1-bb24-455c-84b6-c47225f7b2c6",
   "metadata": {},
   "source": [
    "It helps in dealing with complexities of the text like different word forms, punctuation, and special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74045f23-d33d-4f99-ad1e-b353b43fc9e5",
   "metadata": {},
   "source": [
    "##### Types of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650c607-181b-4bdc-8293-f9ae01dd5ca3",
   "metadata": {},
   "source": [
    "- Word Tokenization: Breaks text into individual words. It's the most common form of tokenization and is usually the first step in text analysis.\n",
    "\n",
    "- Character Tokenization: Breaks text down to its characters. This can be useful for certain languages or when the analysis requires a deeper understanding of the text structure at the character level.\n",
    "\n",
    "- Subword Tokenization: Splits words into smaller meaningful units (subwords). This is useful for handling rare words, or in languages where words are often compounds of smaller units.\n",
    "\n",
    "- Sentence Tokenization: Involves breaking text into individual sentences. This is useful in tasks that require understanding the context at the sentence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adae2e6-1fee-4a46-9a03-22b803c60b8f",
   "metadata": {},
   "source": [
    "##### Types of challenges in Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f55cbc-25ee-41d4-af16-1a4ad6d849eb",
   "metadata": {},
   "source": [
    "- Language Variations: Different languages have different syntactic and grammatical rules, making tokenization a language-dependent task.\n",
    "\n",
    "- Handling Special Cases: Words with apostrophes, hyphens, or special characters can be challenging. For example, deciding whether \"don't\" should be one token or two (\"do\" and \"n't\") depends on the analysis goals.\n",
    "\n",
    "- Contextual Meaning: Tokenization doesnâ€™t consider the meaning of the word in the context, which might be crucial for certain types of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8a690-850e-40e6-b6c3-2245908b5382",
   "metadata": {},
   "source": [
    "#### Hemant thapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73f4faf-30b0-471b-b59c-e28ea37afa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa5969e-1a44-4fd4-8d15-e1bb15041897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of string\n",
    "sentences = [\n",
    "    'I love to read books',\n",
    "    'I love to travel around world',\n",
    "    'Do you love reading books!'  #ddding exclamation mark\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37cd0205-1dcc-4012-990c-44641b267a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love to read books', 'I love to travel around world', 'Do you love reading books!']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1ae386-15b4-4b1f-a396-dee8c6fd83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating model \n",
    "tokenizer = Tokenizer(num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cadcb22-8140-4425-8779-d7260ad96fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model \n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e83ae4-9b55-4ade-93b5-9b8ac0385e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictonary \n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66384903-ccf9-488d-9a28-7da2bf78ab40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'i': 2, 'to': 3, 'books': 4, 'read': 5, 'travel': 6, 'around': 7, 'world': 8, 'do': 9, 'you': 10, 'reading': 11}\n"
     ]
    }
   ],
   "source": [
    "#detected exclamation mark and removed from word books!\n",
    "print(word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
